---
title: 'DATA 698: Masters Research Project'
author: "Gabriella Martinez & Gabriel Campos"
date: "Last edited `r format(Sys.time(), '%B %d, %Y')`"
output:
      html_document:
        toc: true
        toc_depth: 5
        toc_float: yes
        theme: yeti
        highlight: kate
        font-family: "Arial"
        code_folding: show
      html_notebook: default
      pdf_document:
        latex_engine: xelatex
        toc: true
        toc_depth: 5
      geometry: left=0.5cm,right=0.5cm,top=1cm,bottom=2cm
urlcolor: blue
---

# Packages
```{r warning=FALSE, message=FALSE, results='hide'}
#load libraries
library(car)
library(caret)
library(corrplot)
library(ggplot2)
library(janitor)
library(Hmisc)
library(randomForest)
library(reshape2)
library(rvest)
library(tidyverse)
library(tidycensus)
library(httr)
library(xml2)
library(kableExtra)
```

```{r}
# Define the path to the Key folder
api_key_file_path <- file.path(".", "Key", "api_key.txt")

# Read the API key from the file
api_key <- readLines(api_key_file_path, warn = FALSE)

# Print the API key (for debugging purposes; avoid doing this in production)
#cat("API Key:", api_key, "\n")
```

```{r echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
census_api_key(api_key, overwrite = TRUE, install = TRUE)
```

# Data Load

## Election Data

Data was source from [**Harvard Dataverse**](https://data.harvard.edu/dataverse), an open-source data repository platform developed by [Harvard University](https://www.harvard.edu). It is designed to facilitate the sharing, preservation, and citation of research data across various disciplines. Harvard Dataverse is part of the larger Dataverse Project, which provides an open-source platform for institutions to host their own Dataverse installations. The data was extracted to *countypres_2000-2020.csv* and loaded onto our projects github.

```{r, echo=FALSE}
git_url <- "https://raw.githubusercontent.com/gabbypaola/DATA698/refs/heads/main/data/"
```

```{r warning=FALSE}
# Data sourced 
#https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ
# Retrieved from github and stored onto elections dataframe

elect_df <- read_csv(paste0(git_url,"countypres_2000-2020.csv"))

#glimpse(elections)
```

### Data Cleaning (Elections)

```{r}
#identify empty and NA values. 57 NA values in the county_fips column
colSums(elect_df == "" | is.na(elect_df))
```


```{r}
elect_df %>% 
  filter(is.na(county_fips))

elect_df %>%
  filter(is.na(county_fips)) %>%
  select(state_po, county_name, county_fips) %>%
  distinct()
```


```{r}
#clean elections data
elect_data_df <- elect_df %>% 
  #new name = old name
  rename(state_abbr = state_po, pol_identity = party, FIPS = county_fips) %>% 
  mutate(FIPS = ifelse(state_abbr == "DC", "11001", FIPS))

#there are 52 NAs remaining
elect_nas_df <- elect_data_df %>% 
  filter(is.na(FIPS))

elect_nas_df %>% 
  count(state_abbr, county_name) 
```

The remaining **NA** values in the **FIPS** column are votes assigned at a state-wide level, not to any count. The *"MAINE UOCAVA"* county record for the state of Maine represents the count of votes from Uniformed Service & Overseas [**(UOCAVA)**](https://www.maine.gov/sos/cec/elec/voter-info/uocava.html) Voters. The ["STATEWIDE WRITEIN"](https://www.usa.gov/write-in-candidates) for Connecticut represents the count of votes for self-selected candidates not on the presidential ballot. It is unclear what the *"FEDERAL PRECINCT"* for the state of Rhode Island exactly represents. Either way, our analysis will be conducted at the county level, so these records cannot be used. 

Next we will assess the effect that removing these votes will have on our overall analysis.

```{r}
#nas
nrow(elect_nas_df)

# Determine the total number of records in the table.
nrow(elect_nas_df)

round(nrow(elect_nas_df)/nrow(elect_data_df)*100,3)
```

```{r}
# Determine the total number of votes cast across all counties in all elections.
elect_vt_cnt_df <- elect_data_df %>% 
  summarise(count= sum(candidatevotes))

elect_vt_cnt_df
```

```{r}
# Determine how many votes are associated with state-level counts
elect_null_fips_cnt_df <- elect_nas_df %>% 
  summarise(count=sum(candidatevotes))

elect_null_fips_cnt_df

round((elect_null_fips_cnt_df$count/elect_vt_cnt_df$count)*100,3)
```

There were 52 records with state-level counts and null FIPS values in the data, representing 13009 votes.
This amounts to 0.072% of the total records and 0.002% of the total votes.

The records with state-level counts and null FIPS values represent a small percentage of the total, and they are unlikely to change the overall analysis. Given our assessment, the records will be removed.

```{r}
#transform data- drop NAs, keep dem and gop only, group records for each candidate by county and year
elect_cand_vt_df <- elect_data_df %>% 
  filter(!is.na(FIPS), pol_identity %in% c('DEMOCRAT', 'REPUBLICAN')) %>% 
  group_by(FIPS,county_name,
           state, candidate,
           year, pol_identity,
           totalvotes) %>% 
  summarise(candidate_votes = sum(candidatevotes)) %>% 
  ungroup() %>% 
  arrange(FIPS, year)

#spread the candidate votes values
elect_pivot_df <- elect_cand_vt_df %>% 
   pivot_wider(id_cols = c(year, FIPS, county_name, state, totalvotes),
               names_from = pol_identity,
               values_from = candidate_votes) %>% 
  rename(votes_dem = DEMOCRAT,  votes_gop = REPUBLICAN
         #votes_other = OTHER,votes_grn = GREEN, votes_lib = LIBERTARIAN
         )
```


## Census Bureau data

About Census Bureau American Community Survey (ACS) data
https://www.census.gov/programs-surveys/acs/guidance/estimates.html

### Citizen Voting Age Population
Citizen Voting Age Population, Census Bureau population estimates generated using the American Community Survey


```{r, echo=FALSE}
git_url<-"https://raw.githubusercontent.com/GitableGabe/DATA_698/refs/heads/main/data/"
```


```{r, message=FALSE, warning=FALSE, results='hide'}
#CVAP- Citizen Voting Age Population, Census Bureau population estimates
#generated using the American Community Survey

#https://www.census.gov/programs-surveys/decennial-census/about/voting-rights
#/cvap.2010.html#list-tab-1518558936 (2008)
cens_cvap2008 <- 
  read_csv(paste0(git_url,
                  "CountyCVAP_2006-2010.csv",
                  "?token=GHSAT0AAAAAACXYKDAYQCHUVJY2V6BVWU7SZXPAZJQ")) %>% 
  rename_with(tolower) %>% 
  mutate(year=2008)

#https://www.census.gov/programs-surveys/decennial-census/about/voting-rights
#/cvap.2014.html#list-tab-1518558936 (2012)
cens_cvap2012 <- 
  read_csv(paste0(git_url,
           "CountyCVAP_2010-2014.csv",
           "?token=GHSAT0AAAAAACXYKDAYHOL27SGWSEL2AS6IZXPAYSQ")) %>% 
  rename_with(tolower) %>% 
  mutate(year=2012)

#https://www.census.gov/programs-surveys/decennial-census/about/voting-rights
#/cvap/2014-2018-CVAP.html (2016)
cens_cvap2016 <- 
  read_csv(paste0(git_url,
                  "CountyCVAP_2014-2018.csv",
                  "?token=GHSAT0AAAAAACXYKDAZJU7ABMJMRNP5WOSIZXPATUQ")) %>% 
  mutate(year=2016)

#https://www.census.gov/programs-surveys/decennial-census/about/voting-rights
#/cvap/2017-2021-CVAP.html (2020)
cens_cvap2020 <- 
  read_csv(paste0(git_url,
                  "CountyCVAP_2017-2021.csv",
                  "?token=GHSAT0AAAAAACXYKDAYJWVR6SZPSH4NRMSSZXPASSQ")) %>% 
  mutate(year=2020) 

cens_cvap_df <- rbind(cens_cvap2008,
                      cens_cvap2012,
                      cens_cvap2016,
                      cens_cvap2020) %>% 
  filter(lntitle == 'Total', !str_detect(geoname, "Puerto Rico")) %>% 
  mutate(FIPS = str_sub(geoid, -5)) %>% 
  select(c('year', 'FIPS', 'geoname', 'cvap_est'))

#identify empty and NA values
colSums(cens_cvap_df == "" | is.na(cens_cvap_df))
```

#### Merge with Election data

```{r}
vot_info_df <- left_join(elect_pivot_df, cens_cvap_df, by = c("FIPS", "year"))

vot_info_df

ls_states <- sort(str_to_title(unique(vot_info_df$state)))
```

```{r}
#identify empty and NA values
colSums(vot_info_df == "" | is.na(vot_info_df))

vot_info_NAs_df <- vot_info_df %>% 
  filter(is.na(geoname), is.na(cvap_est))

vot_info_NAs_df

unique(vot_info_NAs_df$year)
```


```{r}
vot_info_df <- vot_info_df %>% 
  filter(year >= 2008)

vot_info_NAs_2df <- vot_info_df %>% 
  filter(is.na(geoname), is.na(cvap_est))

vot_info_NAs_2df
```

```{r}
vot_info_df <- vot_info_df %>%
  filter(state != "ALASKA")

vot_info_NAs_3df <- vot_info_df %>%
  filter(is.na(geoname), is.na(cvap_est))

vot_info_NAs_3df
```

```{r}
vot_info_clean_df <- vot_info_df %>% 
  filter(FIPS %in% c('29095', '36000', '51019', '51515')) %>% 
  arrange(year, FIPS)

vot_info_clean_df
```

```{r}
vot_info_clean_df %>% 
  count(FIPS, state, county_name, geoname) %>% 
  filter(geoname %in% c("Jackson County, Missouri", "Bedford County, Virginia")) %>% 
  select(-n)
```


```{r}
# Define the counties to filter and group data by year and state
vot_co_grps_df <- vot_info_df %>%
  filter(FIPS %in% c('29095', '36000', '51019', '51515')) %>%
  group_by(year, state) %>%
  summarise(    # Concatenate FIPS codes and county names
    FIPS = paste(unique(FIPS), collapse = ", "),
    county_name = paste(unique(county_name), collapse = ", "),
            across(where(is.numeric), sum, na.rm = TRUE)) %>% 
  mutate(geoname = case_when(state == "MISSOURI" ~ "Jackson County, Missouri",
                             state == "VIRGINIA" ~ "Bedford County, Virginia"))

vot_co_grps_df
```
#### Clean up

```{r}
#remove the previous observations
vot_info_df <- vot_info_df %>% 
  filter(!FIPS %in% c('29095', '36000', '51019', '51515'))

#replace with the calculated observations
vot_info_df <- rbind(vot_info_df, vot_co_grps_df)


ls_FIPS <- unique(vot_info_df$FIPS)

length(ls_FIPS)

co_names <- vot_info_df %>% 
  group_by(state, county_name) %>% 
  mutate(county_name = str_to_title(county_name),
         state = str_to_title(state)) %>% 
  summarise(n=n())

length(co_names)
```

#### Popular Vote

```{r}
vot_info_df %>% 
  group_by(year) %>% 
  summarise(total_dem = scales::comma(sum(votes_dem)),
            total_gop = scales::comma(sum(votes_gop))) %>% 
  mutate(result = if_else(total_gop > total_dem,
                          "Republican Party","Democratic Party")) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_minimal()
```


```{r}
rm(list = ls(pattern = "^elect_|^cens_"))
```


#### Aggregate by State
```{r}
vot_info_df <- vot_info_df %>% 
  group_by(state, year) %>% 
  summarise(totalvotes = sum(totalvotes),
            votes_dem = sum(votes_dem),
            votes_gop = sum(votes_gop),
            cvap_est = sum(cvap_est)) %>% 
  ungroup() %>% 
  arrange(state, year)

#49 states + DC, Alaska has been removed
length(unique(vot_info_df$state))
```

```{r}
# Assuming your data frame is `state_data`
vot_info_df %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_minimal()

```


#### Calculate additional columns

```{r}
vot_info_fin <- vot_info_df %>% 
  mutate(#voters who did not choose the Democratic or Republican party
         votes_other = totalvotes - votes_dem - votes_gop,
         #voter share attributes
         voter_share_major_party = (votes_dem + votes_gop) / totalvotes,
         voter_share_dem = votes_dem/totalvotes,
         voter_share_gop = votes_gop/totalvotes,
         voter_share_other = votes_other/totalvotes,
         #raw differences
         rawdiff_dem_vs_gop = votes_dem - votes_gop,
         rawdiff_gop_vs_dem = votes_gop - votes_dem,
         rawdiff_dem_vs_other = votes_dem - votes_other,
         rawdiff_gop_vs_other = votes_gop - votes_other,
         rawdiff_other_vs_dem = votes_other - votes_dem,
         rawdiff_other_vs_gop = votes_other - votes_gop,
         #percentage difference
         pctdiff_dem_vs_gop = 
           (votes_dem - votes_gop) / totalvotes,
         pctdiff_gop_vs_dem = 
           (votes_gop - votes_dem) / totalvotes,
         pctdiff_dem_vs_other = 
           (votes_dem - votes_other) / totalvotes,
         pctdiff_gop_vs_other = 
           (votes_gop - votes_other) / totalvotes,
         pctdiff_other_vs_dem = 
           (votes_other - votes_dem) / totalvotes,
         pctdiff_other_vs_gop = 
           (votes_other - votes_gop) / totalvotes,
         #voter turnout
         voter_turnout = totalvotes/cvap_est,
         voter_turnout_majparty = 
           (votes_dem+votes_gop)/cvap_est,
         voter_turnout_dem = votes_dem/cvap_est,
         voter_turnout_gop = votes_gop/cvap_est,
         voter_turnout_other =votes_other/cvap_est,
         # get winning political party
         winning_party = 
           case_when(votes_dem > votes_gop &
                       votes_dem > votes_other ~ "Democratic Party",
                     votes_gop > votes_dem &
                       votes_gop > votes_other ~ "Republican Party",
                     votes_other > votes_dem &
                       votes_other > votes_gop ~ "Other Party"),
         pct_margin_of_victory =
           case_when(winning_party == "Democratic Party" 
                     ~ round(
                       ((votes_dem - votes_gop) / totalvotes)
                       *100,3), #votes_dem > votes_gop
                     winning_party == "Republican Party"
                     ~ round(
                       ((votes_gop - votes_dem) / totalvotes)
                       *100,3), #votes_gop > votes_dem
                     ),
         # create binary outcome version of the variable for model use
         winning_party_binary = 
           case_when(votes_dem > votes_gop & 
                       votes_dem > votes_other ~ 0,
                     votes_gop > votes_dem &
                       votes_gop > votes_other ~ 1,
                     votes_other > votes_dem &
                       votes_other > votes_gop ~ 2),
         ) 

```


```{r, echo=FALSE}
rm(list = ls()[!grepl("vot_info_fin|^ls_", ls())])
```


#### By State Result 
```{r}
vot_info_fin %>% 
  group_by(year, winning_party) %>% 
  summarise(count= n()) %>% 
  pivot_wider(id_cols = year,
              names_from = winning_party,
              values_from = count) %>% 
  mutate(result = case_when(`Republican Party` > `Democratic Party` ~ 
                              "Republican Party",
                            `Democratic Party` > `Republican Party` ~
                              "Democratic Party",
                            `Democratic Party` == `Republican Party` ~
                              "Tie"
                            )
         ) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_minimal()
```


```{r}
summary(vot_info_fin$voter_turnout)
```


```{r}
vot_info_fin <- vot_info_fin %>% 
  mutate(voter_turnout = if_else(voter_turnout>1 , 1, voter_turnout))

summary(vot_info_fin$voter_turnout)
```

```{r}
dim(vot_info_fin)
```

#### Transforming data for modeling

Pivot the table so that each county has one record and so that data for each election is in separate columns.

```{r}
vot_info_fin_pivot <- vot_info_fin %>%
  pivot_wider(
    id_cols = c(state),
    names_from = year,
    values_from = c(totalvotes, cvap_est, voter_turnout, voter_turnout_dem, voter_turnout_gop, pctdiff_dem_vs_gop, rawdiff_dem_vs_gop, 
                    winning_party,winning_party_binary)
  )

dim(vot_info_fin_pivot)

```

```{r}
colSums(is.na(vot_info_fin_pivot))
```

```{r}
vot_info_fin_pivot_na <- vot_info_fin_pivot %>%
  filter(if_any(where(is.numeric), is.na))

vot_info_fin_pivot_na
```


# Exploratory Data Analysis

```{r}
glimpse(vot_info_fin_pivot)
```


```{r}
#identify empty and NA values
colSums(vot_info_fin_pivot == "" | is.na(vot_info_fin_pivot))
```


After cleaning, our dataset includes election data by county for 49 states and the District of Columbia for elections since 2008.

```{r}
vot_info_fin_pivot %>% 
  group_by(state) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count))

```
## Summary Statistics

```{r}
vot_info_fin_pivot %>% 
  # keep(is.numeric) %>% 
  Hmisc::describe()
```


## Distribution of variables

```{r}
# Histograms
vot_info_fin_pivot %>%
  keep(is.numeric) %>% 
  gather() %>%
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density(fill = "#222222", alpha = 0.5, color = "darkgray") +
    geom_histogram(aes(y=..density..), alpha=0.5, fill = "#222222", color="darkgray", position="identity") + 
  theme(axis.title = element_blank())
```


```{r, warning=FALSE, message=FALSE}
vot_info_fin %>%
  group_by(year, winning_party) %>%
  summarise(count = sum(totalvotes)) %>%
  ggplot(aes(x = winning_party, y = count, fill = winning_party)) +  
  # Map fill to winning_party
  scale_fill_manual(values = c("darkblue","red2"))+
  geom_col(width = 0.5) +  #adjust the width as needed
  facet_wrap(~year) +
  theme_bw() + # Setting background as blank
  theme(legend.position = "bottom",
        #legend.position = c(0.11, 0.1), #puts legend inside the plot
        # legend.text = element_text(size = 6), #, family = "Arial"
        legend.key.size = unit(8, "mm"), #changes the size of the legend symbol
        legend.title = element_blank(), #removes legend title
        legend.spacing.x = unit(.25, 'cm'),
        axis.title = element_blank()
        )
```


## Detect Multicollinearity Using Correlation Matrix


```{r}

cor_df <- vot_info_fin_pivot %>% 
  select(-c(state, starts_with("winning"))) %>% 
  keep(is.numeric)

cor_matrix <- cor(cor_df)

# Create a heatmap for the correlation matrix
# Visualize correlation between variables
corrplot.mixed(cor(cor_df %>% keep(is.numeric)),
               tl.col = 'black', tl.pos = 'lt',
               upper = "number", lower="shade",
               shade.col=NA, tl.srt=90 )

```

## Detect Multicollinearity Using VIF

The Variance Inflation Factor (VIF) helps quantify how much multicollinearity exists by showing how much the variance of a coefficient is inflated due to linear dependence with other predictors.

VIF Interpretation:  
VIF = 1: No correlation between the predictor and other variables.  
VIF between 1 and 5: Moderate correlation.  
VIF > 5 (or sometimes > 10): High multicollinearity, and you may want to consider removing this variable.  

```{r}
vif_data <- vif(lm(totalvotes_2020 ~ ., data=cor_df))
# Fit a linear model and calculate VIF
print(vif_data)
```

```{r}
# Convert VIF values to a dataframe for visualization
vif_df <- as.data.frame(vif_data)
vif_df$variables <- rownames(vif_df)
```

# Build Model

Based on the VIF values shown in our exploratory data analysis, it is evident there is high multicollinearity in our data. Multicollinearity, can cause problems in some models (like linear regression) but may not be as critical for tree-based methods like Random Forests. As such, we will build a Random Forest Model. 

Before modelling, we will exclude non-predictive columns like 'FIPS', 'county', and 'state' from the model and subset the data to only include relevant columns. The columns "FIPS", "county", and "state" are identifiers or categorical labels, not numerical values that contribute directly to predicting totalvotes_2020. Including categorical variables like "county" or "state" without encoding them properly can lead to high dimensionality when creating dummy variables.

## Base model
### Train
```{r}
#train
df_subset <- vot_info_fin_pivot %>% 
  select(-c("winning_party_2008",
            "winning_party_2012",
            "winning_party_2020",
            "winning_party_2016")) %>% 
  mutate(across(starts_with("winning"), as.factor),
         state = as.factor(state))
       
# Split the data into training and testing sets (70% train, 30% test)
set.seed(123)  # for reproducibility
train_indices <- sample(seq_len(nrow(df_subset)),
                        size = 0.7 * nrow(df_subset))
train_data <- df_subset[train_indices, ]
test_data <- df_subset[-train_indices, ]

rf_model <- randomForest(winning_party_binary_2020 ~ .,
                         data = train_data, ntree = 500,
                         mtry = 5, importance = TRUE)

# View the model summary
print(rf_model)
    
```

This is the out-of-bag (OOB) error estimate, which is an internal error estimate in random forests. In this case, the OOB error rate is 2.86%, meaning that the model predicts strongly on the training data based on the OOB observations. Overall, the model proves to be highly accurate with almost perfect results and minimal overfitting. 

### Evaluate
```{r}
#evaluate
# Predictions on the test data
predictions <- predict(rf_model, test_data)

table(predictions)

# Confusion matrix to evaluate accuracy
conf_matrix <- confusionMatrix(predictions,
                               test_data$winning_party_binary_2020)
print(conf_matrix)

```

```{r, echo=FALSE}
rm(list = ls(pattern = "^cor|df_subset|^conf"))
```

The test data correctly predicts Democrat Party for the 2020 election. 

8 samples were correctly classified as 0 (True Negatives).
6 samples were correctly classified as 1 (True Positives).
1 sample was misclassified as 1 instead of 0 (False Positive).
0 samples were misclassified as 0 instead of 1 (False Negative).

Accuracy is the proportion of correct predictions over the total number of predictions:
Accuracy =8+6/(8+6+1+0) = 0.9333 or 93.33%
This indicates the model correctly classified 93.33% of the test data.

### Checking for Overfitting
```{r}
rf_cv <- train(winning_party_binary_2020 ~ .,
               data = train_data, method = "rf",
               trControl = trainControl(method = "cv",
                                        number = 10))

print(rf_cv)
```

This Random Forest model shows good performance on the dataset (up to 93.3% accuracy). The tuning process optimized the mtry parameter to balance model complexity and predictive performance. With mtry = 41, the model uses a significant portion of the predictors for splitting, which is likely appropriate given the relatively small number of samples.

If deployed, the model should generalize well given the robustness of Random Forest and the cross-validation methodology used.

## Demographic data

```{r, warning= FALSE, message=FALSE, results='hide'}
# To obtain data for the 2008 population from the American Community 
# Survey (ACS), you should use the 2006-2008 ACS 3-Year Estimates. 
# This dataset aggregates data collected over those three years,
#  providing insights for the population during that period. 5 
# year ACS data unavailable for 2008. 3 year ACS data was discontinued
# after 2009.

#load 2008 data using API
ed_attain2008 <- get_acs(
  geography = "county",
  variables = c(paste0("B15001_00",
                       seq(01,09),"E"),
                paste0("B15001_0",
                       seq(10,83),"E")),
  year = 2008,
  survey = "acs3",
  cache_table = TRUE) %>%
  mutate(year=2008)

#2012 data and onward uses the 5 year ACS data
#load 2012 data using API
ed_attain2012 <- get_acs(
  geography = "county",
  variables = c(paste0("B15001_00",
                       seq(01,09),"E"),
                paste0("B15001_0",
                       seq(10,83),"E")),
  year = 2012,
  survey = "acs5",
  cache_table = TRUE) %>%
  mutate(year=2012)

#load 2016 data using API
ed_attain2016 <- get_acs(
  geography = "county",
  variables = c(paste0("B15001_00",
                       seq(01,09),"E"),
                paste0("B15001_0",
                       seq(10,83),"E")),
  year = 2016,
  survey = "acs5",
  cache_table = TRUE) %>%
  mutate(year=2016)

#load 2020 data using API
ed_attain2020 <- get_acs(
  geography = "county",
  variables = c(paste0("B15001_00",
                       seq(01,09),"E"),
                paste0("B15001_0",
                       seq(10,83),"E")),
  year = 2020,
  survey = "acs5",
  cache_table = TRUE) %>%
  mutate(year=2020)

```
#### Get column names

```{r}
#check column names
#get column names 2008
url08 <- "https://api.census.gov/data/2008/acs/acs3/groups/B15001.html"

webpage08 <- read_html(url08)

table08 <- webpage08 %>%
  html_node("table") %>%  # Adjust the selector if necessary
  html_table() %>%
  select(c("Name","Label","Concept","Required","Attributes",
           "Limit","Predicate Type","Group"))

filteredtable08 <- table08 %>%
  # filter(!is.na(Name) & Name != "") %>%  
  # Remove rows with NA or empty names
  filter(Name %in% c(paste0("B15001_00", seq(01,09),"E"),
                     paste0("B15001_0", seq(10,83),"E")))
# %>%
#    mutate(Label = str_replace_all(Label,", GED, or alternative",
# ' (includes equivalency)'))

#get column names  2012
url12 <- "https://api.census.gov/data/2012/acs/acs5/groups/B15001.html"

webpage12 <- read_html(url12)

table12 <- webpage12 %>%
  html_node("table") %>%  # Adjust the selector if necessary
  html_table() %>%
  select(c("Name","Label","Concept","Required","Attributes",
           "Limit","Predicate Type","Group"))

filteredtable12 <- table12 %>%
  # filter(!is.na(Name) & Name != "") %>%  
  # Remove rows with NA or empty names
  filter(Name %in% c(paste0("B15001_00", seq(01,09),"E"),
                     paste0("B15001_0", seq(10,83),"E")))
# %>%
#    mutate(Label = str_replace_all(Label,", GED, or alternative",
#' (includes equivalency)'))

#get column names 2016
url16 <- "https://api.census.gov/data/2016/acs/acs5/groups/B15001.html"

webpage16 <- read_html(url16)

table16 <- webpage16 %>%
  html_node("table") %>%  # Adjust the selector if necessary
  html_table() %>%
  select(c("Name","Label","Concept","Required","Attributes",
           "Limit","Predicate Type","Group"))

filteredtable16 <- table16 %>%
  # filter(!is.na(Name) & Name != "") %>%  # Remove rows with NA or empty names
  filter(Name %in% c(paste0("B15001_00", seq(01,09),"E"),
                     paste0("B15001_0", seq(10,83),"E")))

#get columnn names 2020
url20 <- "https://api.census.gov/data/2020/acs/acs5/groups/B15001.html"

webpage20 <- read_html(url20)

table20 <- webpage20 %>%
  html_node("table") %>%  # Adjust the selector if necessary
  html_table() %>%
  select(c("Name","Label","Concept","Required","Attributes",
           "Limit","Predicate Type","Group"))

filteredtable20 <- table20 %>%
  # filter(!is.na(Name) & Name != "") %>%  # Remove rows with NA or empty names
  filter(Name %in% c(paste0("B15001_00", seq(01,09),"E"),
                     paste0("B15001_0", seq(10,83),"E"))) %>%
  mutate(Label = str_replace_all(Label,":",""))

```

```{r message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#Are column names the same across all ACS data?

table(filteredtable08==filteredtable12)

table(filteredtable08==filteredtable16)

table(filteredtable08==filteredtable20)

table(filteredtable12==filteredtable16)

table(filteredtable12==filteredtable20)

table(filteredtable16==filteredtable20)
```

```{r}
#update the mismatches
filteredtable08 <- filteredtable08 %>%
   mutate(Label = str_replace_all(Label,", GED, or alternative",
                                  ' (includes equivalency)'))

filteredtable12 <- filteredtable12 %>%
  mutate(Label = str_replace_all(Label,", GED, or alternative",
                                 ' (includes equivalency)'))
```

```{r message=FALSE, warning=FALSE, include=FALSE, results='hide'}
#recheck
#Are column names the same across all ACS data?

table(filteredtable08==filteredtable12)

table(filteredtable08==filteredtable16)

table(filteredtable08==filteredtable20)

table(filteredtable12==filteredtable16)

table(filteredtable12==filteredtable20)

table(filteredtable16==filteredtable20)

#yes, they are now so we can just use the column names from the latest ACS
```

All column names are the same across all 4 election year Educational Attainment data.

#### Combine and merge education data
```{r}
ed_attain <- rbind(ed_attain2008, ed_attain2012, ed_attain2016, ed_attain2020)

```

```{r}
ed_colnames <- filteredtable20 %>%
  mutate(Name = str_replace_all(Name,"E","")) %>%
  select(c(Name, Label))

table(sort(unique(ed_colnames$Name))==sort(unique(ed_attain$variable)))

ed_attain2a <- left_join(ed_attain, ed_colnames, by = c("variable"="Name"))

glimpse(ed_attain2a)
```

```{r, echo=FALSE}
rm(list = ls(pattern = "^ed_attain20|^webpage|^url|^filtered"))
```



```{r}
#identify empty and NA values
colSums(ed_attain2a == "" | is.na(ed_attain2a))
```
#### Clean and reshape data

```{r}
# voteFIPS <- unique(voting_info_final_pivot$FIPS)
demoFIPS <- unique(ed_attain2a$GEOID)

ed_attain2 <- ed_attain2a %>%
  filter(!GEOID %in% setdiff(demoFIPS, ls_FIPS)) %>% 
  #keep only the fips we have in the voting dataset
  separate(col="NAME", into=c("county", "state"), sep=",") %>% 
  mutate(county = str_remove(county, " County"),
         county = if_else(county == "Doña Ana", "Dona Ana", county)
         )

ed_attain3 <- ed_attain2 %>% 
  group_by(state, year, variable, Label) %>% 
  summarise(estimate = sum(estimate),
            moe = sum(moe)) %>% 
  mutate(Label2 = Label) %>%
  separate(Label2, into = c("type","value","gender", "age_group",
                            "education"), sep = "!!") 
  
length(unique(ed_attain3$GEOID))

# edcountystate <- ed_attain3 %>% 
#   select(GEOID,county, state) %>% 
#   distinct(GEOID,county,state) %>%
#   group_by(GEOID) %>%
#   summarise(count=n())

```

```{r}
head(ed_attain3, 10)
```

```{r}
#identify empty and NA values
colSums(ed_attain3 == "" | is.na(ed_attain3))
```


```{r}
ed_attain3_na <- ed_attain3 %>%
  filter(is.na(gender) | is.na(age_group) |
           is.na(education)) #is.na(gender) | 

ed_attain3_na %>%
  count(variable, Label)

unique(ed_attain3_na$variable)

```

```{r}
#total county population
tot_pop <- ed_attain3 %>%
  filter(is.na(gender)) %>% 
  select(state,  estimate, year, value) 
#value is the column name that will be used to spread/pivot_wider

#male/female county population
gen <- ed_attain3 %>%
  filter(is.na(age_group), !is.na(gender)) %>% 
  select(state,  estimate, year, gender)

#gender and age grp population
age_gen_pop <- ed_attain3_na %>% 
  filter(!is.na(age_group)) %>% 
  select(state,  estimate, year, gender, age_group)

#gender, age, education
ed_pop <- ed_attain3 %>% 
  filter(!is.na(education)) %>% 
  select(state, estimate, year, gender, age_group, education) 

#age, education
age <- ed_pop %>% 
  group_by(state,  year, age_group) %>% 
  summarise(estimate = sum(estimate))

#gender, education
ed_pop2 <- ed_pop %>% 
  group_by(state,  year, gender, education) %>% 
  summarise(estimate = sum(estimate))
  
#age, education
ed_pop3 <- ed_pop %>% 
  group_by(state,  year, age_group,  education) %>% 
  summarise(estimate = sum(estimate))

#education
ed_pop4 <- ed_pop %>% 
  group_by(state,  year, education) %>% 
  summarise(estimate = sum(estimate))

```

### Age, Gender, Education

```{r}
#need to spread/pivot_wider and then merge with main dataset for modelling 
#age
age <- ed_pop %>% 
  group_by(state,  year, age_group) %>% 
  summarise(estimate = sum(estimate))

#gender
gen <- ed_attain3 %>%
  filter(is.na(age_group), !is.na(gender)) %>% 
  select(state,  estimate, year, gender)

#education level
edu <- ed_pop %>% 
  group_by(state,  year, education) %>% 
  summarise(estimate = sum(estimate))
```
```{r}
#age pivoted
age2 <- age %>% 
  pivot_wider(id_cols = c(state),
              names_from = c(year,age_group),
              values_from = estimate) 

colSums(age2 == "" | is.na(age2))

#gender pivoted
gen2 <- gen %>% 
  pivot_wider(id_cols = c(state),
              names_from = c(year, gender),
              values_from = estimate)

colSums(gen2 == "" | is.na(gen2))

#edu pivoted
edu2 <- edu %>% 
  pivot_wider(id_cols = c(state),
              names_from = c(year, education),
              values_from = estimate)

colSums(edu2 == "" | is.na(edu2))

age2 <- age2 %>% 
  select(-starts_with("2008"))

gen2 <- gen2 %>% 
  select(-starts_with("2008"))

edu2 <- edu2 %>% 
  select(-starts_with("2008"))
```



```{r}
dem0 <- left_join(age2, gen2, by = c("state"))

dem <- left_join(dem0, edu2, by = c("state")) %>% 
  ungroup()

#check dimensions, there is an extra state now
dim(dem)
```


```{r}
#na / empty cell check
colSums(dem == "" | is.na(dem))

#check for dupe, no dupe, but Puerto Rico needs to be filtered out
unique(dem$state)
```
#### Clean up
```{r}
dem <- dem %>% 
  filter(!str_detect(state, "Puerto Rico")) %>% 
  mutate(state = trimws(state, which="both"))

vot_info_fin_pivot <- vot_info_fin_pivot %>% 
  mutate(state = str_to_title(state))
```


### Merge with model data
```{r}
model_data <- left_join(vot_info_fin_pivot, dem, join_by(state == state))

dim(model_data)

colSums(model_data == "" | is.na(model_data))

model_data2 <- model_data %>% 
  drop_na() %>% 
  janitor::clean_names()

dim(model_data2)
```

#Build Second Model 
### Train
```{r}
#train
df_subset2 <- model_data2 %>% 
  select(-c("winning_party_2008", "winning_party_2012", "winning_party_2020", "winning_party_2016")) %>% 
  mutate(across(starts_with("winning"), as.factor),
         state = as.factor(state))
       
# Split the data into training and testing sets (70% train, 30% test)
set.seed(123)  # for reproducibility
train_indices2 <- sample(seq_len(nrow(df_subset2)), 
                         size = 0.7 * nrow(df_subset2))
train_data2 <- df_subset2[train_indices2, ]
test_data2 <- df_subset2[-train_indices2, ]

rf_model2 <- randomForest(winning_party_binary_2020 ~ .,
                          data = train_data2,
                          ntree = 500, 
                          mtry = 5,
                          importance = TRUE)

# View the model summary
print(rf_model2)
    
```

True 0 (15): 15 instances of class 0 were correctly classified.  

False 0 (1): 1 instance was incorrectly classified as 0.  

True 1 (17): 17 instances of class 1 were correctly classified.  

False 1 (1): Only 1 instance was incorrectly classified as 1.  
  
Class error:  
For class 0: 0.0625% error.  
For class 1: 0.0556% error.

### Evaluate
```{r}
#evaluate
# Predictions on the test data
predictions2 <- predict(rf_model2, test_data2)

#0= dem, 1=rep
table(predictions2)

# Confusion matrix to evaluate accuracy
conf_matrix2 <- confusionMatrix(predictions2, test_data2$winning_party_binary_2020)
print(conf_matrix2)

```

The model performs well overall, with high accuracy (93.33%), excellent sensitivity (88.89%), and perfect specificity (100%). It is also statistically significantly better than random predictions (p = 0.005172). It missed only one instance where the true class was 1 but predicted as 0.  

### Checking for Overfitting
```{r}
rf_cv2 <- train(winning_party_binary_2020 ~ .,
                data = train_data2,
                method = "rf",
                trControl = trainControl(method = "cv", number = 10))

print(rf_cv2)
```

# Prediction
```{r}
predictions_2024 <- predict(rf_model2, df_subset2)

#demo = 0, rep = 1
table(predictions_2024) # Republican Party

# table(df_subset2$winning_party_binary_2020) #Democratic Party
# 
# table(df_subset2$winning_party_binary_2016) #Republican Party

```
The prediction results of the model show that the Republican Party would win the 2024 elections which is true to the outcome of our elections this year.

## Model predictions by state
```{r}
#merge predictions back with original data 
model_data3 <- model_data2

model_data3$predicted_values2024 <- predictions_2024

model_data3 <- model_data3 %>% 
  mutate(winning_prediction_2024 = if_else(predictions_2024 == 0, "Democratic Party", "Republican Party"))

state_predictions <- model_data3 %>% 
  select(c(state, winning_prediction_2024))
```


## Actual election results by state
```{r}
# Specify the URL
url <- "https://www.reuters.com/graphics/USA-ELECTION/RESULTS/zjpqnemxwvx/"

response <- GET(url)

# Parse the webpage content
webpage <- read_html(content(response, as = "text"))

# Extract the table(s)
tables <- html_table(webpage, fill = TRUE)

actual_results2024 <- rbind(tables[[1]],tables[[2]],tables[[3]],tables[[4]],tables[[5]])

colnames(actual_results2024)[colnames(actual_results2024) == ""] <- "st_abbrv"

actual_results2024_ <- actual_results2024 %>% 
  filter(!st_abbrv == "") %>% 
  mutate(st_abbrv = case_when(st_abbrv=="D.C." ~"District Of Columbia",
                              st_abbrv == "Md." ~ "Maryland",
                              st_abbrv == "Neb." ~ "Nebraska",
                              st_abbrv == "N.C." ~ "North Carolina",
                              st_abbrv == "N.D." ~ "North Dakota",
                              st_abbrv == "N.H." ~ "New Hampshire",
                              st_abbrv == "N.J." ~ "New Jersey",
                              st_abbrv == "N.M." ~ "New Mexico",
                              st_abbrv == "N.Y." ~ "New York",
                              st_abbrv == "Nev." ~ "Nevada",
                              st_abbrv == "Va." ~ "Virginia",
                              st_abbrv == "Vt." ~ "Vermont",
                              st_abbrv == "W.Va." ~ "West Virginia",
                              st_abbrv == "Wash." ~ "Washington",
                              TRUE ~ st_abbrv)) %>% 
  arrange(st_abbrv) %>% 
  mutate(State = ls_states,
         Democrat = as.numeric(str_remove(Dem., "%"))/100,
         Republican = as.numeric(str_remove(Rep., "%"))/100,
         winning_party = if_else(Democrat>Republican, "Democratic Party","Republican Party")
         )

act_res24_tbl <- actual_results2024_ %>% 
  select(c(State, Democrat, Republican, winning_party)) 

act_res24_tbl %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_minimal()

```


<!--  -->


<!-- ### Feature Importance -->

<!-- ```{r} -->
<!-- # Variable importance -->
<!-- #varImpPlot(rf_model) -->
<!-- ImpData <- as.data.frame(importance(rf_model)) -->
<!-- ImpData$Var.Names <- row.names(ImpData) -->

<!-- #reorder variables based on MeanDecreaseAccuracy to display in descending order -->
<!-- ImpData$Var.Names <- factor(ImpData$Var.Names, levels = ImpData$Var.Names[order(ImpData$MeanDecreaseAccuracy, decreasing = FALSE)]) -->

<!-- ggplot(ImpData, aes(x=Var.Names, y=MeanDecreaseAccuracy)) + -->
<!--   geom_segment(aes(x=Var.Names, xend=Var.Names, y=0, yend=MeanDecreaseAccuracy), color="skyblue") + -->
<!--   #geom_point(aes(size = IncNodePurity), color="steelblue", alpha=1) + -->
<!--   theme_light() + -->
<!--   coord_flip() + -->
<!--   theme( -->
<!--     legend.position = "bottom", -->
<!--     panel.grid.major.y = element_blank(), -->
<!--     panel.border = element_blank(), -->
<!--     axis.ticks.y = element_blank() -->
<!--   ) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- #reorder variables based on MeanDecreaseGini to display in descending order -->
<!-- ImpData$Var.Names2 <- factor(ImpData$Var.Names, levels = ImpData$Var.Names[order(ImpData$MeanDecreaseGini, decreasing = FALSE)]) -->

<!-- ggplot(ImpData, aes(x=Var.Names2, y=MeanDecreaseGini)) + -->
<!--   geom_segment(aes(x=Var.Names2, xend=Var.Names2, y=0, yend=MeanDecreaseGini), color="skyblue") + -->
<!--   #geom_point(aes(size = IncNodePurity), color="steelblue", alpha=1) + -->
<!--   theme_light() + -->
<!--   coord_flip() + -->
<!--   theme( -->
<!--     legend.position = "bottom", -->
<!--     panel.grid.major.y = element_blank(), -->
<!--     panel.border = element_blank(), -->
<!--     axis.ticks.y = element_blank() -->
<!--   ) -->
<!-- ``` -->



<!-- Other references and notes   -->


<!-- Esri prompt   -->
<!-- Because voting is voluntary in the United States, the level of voter participation -->
<!-- (referred to as "voter turnout") has a significant impact on the election results -->
<!-- and resulting public policy.   -->
<!-- Modeling voter turnout, and understanding where low turnout is prevalent, can -->
<!-- inform outreach efforts to increase voter participation. With the ultimate goal -->
<!-- of predicting voter turnout, in this exercise, you will focus on performing various -->
<!-- data engineering tasks to prepare election result data for predictive analysis. -->


<!-- Voter turnout   -->
<!-- https://electionlab.mit.edu/research/voter-turnout   -->
<!-- https://election.lab.ufl.edu/ (VEP- Voting Eligible Population) data by state and election, by county not available   -->

<!-- Redistricting    -->
<!-- https://www.propublica.org/article/redistricting-a-devils-dictionary (definitions)   -->

<!-- Class bias   -->
<!-- https://web.archive.org/web/20141107053600/http://academic.udayton.edu/grantneeley/pol%20303/avery%20and%20peffley%20-%20SPPQ%202005.pdf   -->
<!-- https://web.archive.org/web/20160412151014/https://www.nyu.edu/gsas/dept/politics/faculty/nagler/apsa2006_rv7.pdf   -->

<!-- Felon disenfranchisement   -->
<!-- https://web.archive.org/web/20190516191656/https://felonvoting.procon.org/sourcefiles/uggen_manza_asr_02.pdf   -->


<!------- Below is for removing excessive space in Rmarkdown | HTML formatting -------->

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>